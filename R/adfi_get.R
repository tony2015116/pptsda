# WARNING - Generated by {fusen} from dev/flat_teaching.Rmd: do not edit by hand

#' ADFI get from pig performance test station csv data
#' 
#' @param data A data frame or data table containing the nedap or fire pig performance test data to be processed. Columns must include 'visit_time', 'location', 'responder', 'feed_intake'.
#' @param adg_res ADG results from function of adg_get().
#'
#' @return A list containing:
#' - dfi_info: A data.table containing DFI statistics
#' - dfi_data: A data.table containing processed sample data
#' 
#' @importFrom data.table ":=" ".SD" ".GRP" ".N"
#' 
#' @export
#' @examples
#' nedap_csv_data <- mintyr::nedap
#' adg_results <- adg_get(data = nedap_csv_data)
#' adfi_results <- adfi_get(data = nedap_csv_data, adg_res = adg_results)
#' head(adfi_results$adfi_info)

# get adfi results
adfi_get <- function(data, adg_res) {
  # prepare dfi data
  prepare_dfi_data <- process_adg_data(data) |>
    process_dfi_data()
  
  # prepare about error types
  prepare_error_types <- error_prop_get(prepare_dfi_data)
  
  # prepare scaled data
  scale_data <- prepare_scale_data(right_dfi = prepare_error_types$right_dfi, error_prop = prepare_error_types$error_prop_result, adg_info = adg_res$adg_info, adg_data = adg_res$adg_data)
  
  # run model and merge model predict results
  merge_model_results <- merge_lmm_results(data = scale_data$processed_data, error_dfi = prepare_error_types$error_dfi, right_dfi_each_day = scale_data$right_dfi_each_day)
  
  # get adfi results
  adfi_results <- calculate_adfi(merge_model_results, adg_info = adg_res$adg_info, adg_data = adg_res$adg_data, feedintake_order = prepare_dfi_data$feedintake_order, origin_dfi = prepare_error_types$origin_dfi)

  return(adfi_results)
}
# dfi data transform
process_dfi_data <- function(data) {
  # Make a copy of the input data to preserve the original dataset.
  data_pre <- data.table::copy(data)

  # Select specific columns from the data table.
  data_pre[, .(responder, location, date, seq_in_location, seq_days, seq_in_day, duration, visit_time, feed_intake, weight)]

  # Split the dataset by 'location' into a list of data tables.
  data_pre2 = split(data_pre, by = "location")

  # Apply a function to each element of the list to process and rename columns.
  data_pre3 = purrr::map(data_pre2, \(x) {
    temp = x[order(visit_time)][responder != 0 & !is.na(location)]
    data.table::setnames(temp, c("feed_intake", "duration", "visit_time"),
                         c("fiv", "otv", "entrancetime"))
    temp[, `:=`(entrancefeedweight = 0, exitfeedweight = 0,
                exittime = entrancetime + as.integer(otv), #lubridate::seconds(otv)
                frv = fiv/(otv/60))][, `:=`(lwd = 0, fwd = 0,
                                            ltd = 0, ftd = 0)]
  })

  # Print messages indicating successful generation of variables.
  message(crayon::cyan("\u2022 Successfully generated the following 3 variables:"))
  message(crayon::green(" - FIV:feed intake per visit;"))
  message(crayon::green(" - OTV:occupation time per visit;"))
  message(crayon::green(" - FRV:feeding rate per visit;"))
  # More variables are commented out for future use or debugging.

  # Further processing to identify outlier and erroneous data entries.
  transformed_1 <- purrr::map(data_pre3, \(x) {
    x[, `:=`(
      fiv_lo = data.table::fifelse(fiv < -20, 1, 0),
      fiv_hi = data.table::fifelse(fiv > 2000, 1, 0),
      fiv_0 = data.table::fifelse(otv == 0 & abs(fiv) > 20, 1, 0),
      otv_lo = data.table::fifelse(otv < 0, 1, 0),
      otv_hi = data.table::fifelse(otv > 3600, 1, 0),
      frv_hi_fiv_lo = data.table::fifelse(fiv > 0 & fiv < 50 & frv > 500, 1, 0),
      frv_hi_strict = data.table::fifelse(fiv >= 50 & any(data.table::shift(fiv, type = "lag") < -20, data.table::shift(fiv, type = "lead") < -20) & frv > 110, 1, 0),
      frv_hi = data.table::fifelse(fiv >= 50 & any(data.table::shift(fiv, type = "lag", n = 2) < -20, data.table::shift(fiv, type = "lead", n = 2) < -20) & frv > 170, 1, 0),
      frv_0 = data.table::fifelse(frv == 0 & otv > 500, 1, 0),
      frv_lo = data.table::fifelse(frv != 0 & abs(frv) <= 2, 1, 0),
      lwd_lo = data.table::fifelse((!is.na(lwd)) & lwd < -20, 1, 0),
      lwd_hi = data.table::fifelse((!is.na(lwd)) & lwd > 1800, 1, 0),
      fwd_lo = data.table::fifelse((!is.na(fwd)) & fwd < -20, 1, 0),
      fwd_hi = data.table::fifelse((!is.na(fwd)) & fwd > 1800, 1, 0),
      ltd_lo = data.table::fifelse((!is.na(ltd)) & ltd < 0, 1, 0),
      ftd_lo = data.table::fifelse((!is.na(ftd)) & ftd < 0, 1, 0)
    )]
  })
  transformed_2 = data.table::rbindlist(transformed_1, use.names = T, fill = T)

  # Print a message indicating the number of error types generated from variables.
  message(crayon::green("\u2022 Successfully generated 10 error types from 3 variables:"))
  message(crayon::green(" - FIV-lo; FIV-hi; FIV-0; OTV-lo; OTV-hi; FRV-hi-FIV-lo; FRV-hi-strict; FRV-hi; FRV-0; FRV-lo;"))
  # Additional error types are commented out.

  # Aggregate and organize the final transformed data for further analysis.
  transformed_3 <- transformed_2[, .(date, seq_in_day, seq_days, location, responder, entrancetime, exittime, entrancefeedweight, exitfeedweight, weight, otv, fiv, frv, ltd, ftd, lwd, fwd, fiv_lo, fiv_hi, fiv_0, otv_lo, otv_hi, frv_hi_fiv_lo, frv_hi_strict, frv_hi, frv_0, frv_lo, lwd_lo, lwd_hi, fwd_lo, fwd_hi, ltd_lo, ftd_lo)]

  # Extract feed intake data with order.
  extract_feed_intake_order <- transformed_2[, .(responder, location, date, seq_in_location, seq_days, seq_in_day, fiv, weight)]

  # Prepare final data as a list containing transformed data and feed intake order.
  final <- list(transformed_data = transformed_3, feedintake_order = extract_feed_intake_order)

  # Return the final list containing all processed data.
  return(final)
}
# create error types and error types' prop
error_prop_get <- function(data) {
  # Define error types
  ERROR_TYPES <- c("fiv_lo", "fiv_hi", "fiv_0", "otv_lo", "otv_hi", "frv_hi_fiv_lo", "frv_hi_strict", "frv_hi", "frv_0",
                   "frv_lo", "lwd_lo", "lwd_hi", "fwd_lo", "fwd_hi", "ltd_lo", "ftd_lo")

  # Select required columns and calculate total errors (OE)
  processed_data <- data$transformed_data[, c("date", "seq_in_day", "seq_days", "location", "responder", "weight",
                                              "otv", "fiv", "frv", "ltd", "ftd", "lwd", "fwd",
                                              ..ERROR_TYPES)]
  processed_data[, OE := rowSums(.SD, na.rm = TRUE), .SDcols = ERROR_TYPES]

  # Calculate original Daily Feed Intake (DFI)
  origin_dfi <- processed_data[, .(origin_dfi = sum(fiv)), by = .(responder, seq_days)]

  # Calculate correct DFI part
  right_dfi <- processed_data[OE == 0, .(dfi_right_part = sum(fiv)), by = .(responder, seq_days)]

  # Extract error DFI data
  error_dfi_data <- processed_data[OE != 0]
  error_dfi <- error_dfi_data[, .(dfi_error_part = sum(fiv)), by = .(responder, seq_days)]

  # Function to transform OTD and FID data
  otd_fid_trans <- function(data, target_col, error_cols, new_names, ...) {
    # Inner function to sum target column for rows where error column > 0
    temp1 <- function(col_names, ...) {
      eval(as.name(data))[eval(as.name(col_names)) > 0, by = .(responder, seq_days), purrr::map(.SD, sum), .SDcols = target_col]
    }

    # Apply temp1 function to each error column
    temp2 <- purrr::map(error_cols, temp1)

    # Rename the summed columns with new names
    temp3 <- purrr::map2(temp2, new_names, function(x, y) data.table::setnames(x, target_col, y))

    # Merge all the resulting data tables
    temp4 <- Reduce(function(x, y) merge(x, y, by = c("responder", "seq_days"), all = TRUE), temp3)

    return(temp4)
  }

  # Create a data table with parameters for OTD and FID transformations
  useful_list <- data.table::data.table(
    data = c("error_dfi_data", "error_dfi_data"),
    target_col = c("otv", "fiv"),
    error_cols = list(I(names(error_dfi_data)[c(14:15, 19:27)]), I(names(error_dfi_data)[c(17:18, 28:29)])),
    new_names = list(I(paste0("otd_", c(1:2, 6:14))), I(paste0("fid_", c(4:5, 15:16))))
  )

  # Apply the otd_fid_trans function to each row of useful_list
  otd_fid_result <- purrr::pmap(useful_list, otd_fid_trans)

  # Merge OTD and FID results
  merged_result <- Reduce(function(x, y) merge(x, y, by = c("responder", "seq_days"), all = TRUE), otd_fid_result)

  # Calculate error type proportions
  error_cols <- names(processed_data)[14:29]
  error_proportions <- processed_data[, c(.N, lapply(.SD, sum)), .SDcols = error_cols, by = .(responder, seq_days)]
  error_proportions[, (paste0(error_cols, "_p")) := lapply(.SD, function(x) x / N), .SDcols = error_cols, by = .(responder, seq_days)]
  error_proportions[, c(error_cols, "N") := NULL]

  # Merge all results
  final_result <- merge(merged_result, error_proportions, by = c("responder", "seq_days"), all.x = TRUE)
  final_result <- merge(final_result, error_dfi, by = c("responder", "seq_days"), all.x = TRUE)

  # Replace NA with 0
  numeric_cols <- names(final_result)[sapply(final_result, is.numeric)]
  data.table::setnafill(final_result, fill = 0, cols = numeric_cols)

  final <- list(error_prop_result = final_result, origin_dfi = origin_dfi, right_dfi = right_dfi, error_dfi = error_dfi)

  return(final)
}
# prepare scale data
prepare_scale_data <- function(right_dfi, error_prop, adg_info, adg_data) {
  # Extract right DFI data for days with errors
  right_dfi_in_one_day <- error_prop[right_dfi, on = .(responder, seq_days)][!is.na(dfi_error_part)][, dfi_error_part := NULL]

  # Extract right DFI data for days without errors
  right_dfi_each_day <- error_prop[right_dfi, on = .(responder, seq_days)][is.na(dfi_error_part)][, .(responder, seq_days, dfi_right_part)]

  # Extract base info for ADG (Average Daily Gain)
  base_info_adg <- adg_info[, .(responder, location, lm_slope)]

  # Extract and calculate average weight for each responder and day
  base_info_weight <- unique(adg_data[, weight := as.double(weight)][, .(responder, seq_days, weight)][, weight := mean(weight, na.rm = TRUE), by = .(responder, seq_days)][])

  # Merge DFI data with ADG info
  merged_data <- merge(right_dfi_in_one_day, base_info_adg, by = "responder", all.x = TRUE)

  # Merge with weight info
  merged_data <- merge(merged_data, base_info_weight, by = c("responder", "seq_days"), all.x = TRUE)

  # Remove rows with NA weight
  merged_data <- merged_data[!is.na(weight)]#[, .(LWD-lo, LWD-hi, FWD-lo, FWD-hi, LTD-lo, FTD-lo) := NULL]

  # Process data using recipes package
  processed_data <- data.table::setDF(merged_data) |>
    recipes::recipe(dfi_right_part ~ .) |>
    recipes::update_role(responder, seq_days, location, new_role = "id") |>
    recipes::step_zv(recipes::all_numeric()) |>
    recipes::step_corr(recipes::all_predictors(), threshold = 0.9) |>
    #recipes::step_zv(recipes::all_numeric()) |>
    recipes::step_scale(recipes::all_predictors()) |>
    recipes::prep() |>
    recipes::juice() #|>
    #dplyr::mutate_at("responder", as.factor)
  processed_data$responder <- as.factor(processed_data$responder)

  final <- list(processed_data = processed_data, right_dfi_each_day = right_dfi_each_day)

  return(final)
}
# create linear mixed model
run_lmm_models <- function(data, predictors1, predictors2) {
  # Helper function to format and print equation
  print_equation <- function(eq) {
    formula_str <- paste0(crayon::cyan("\u2022 Running linear mixed model with equation: \n"), " ", deparse(eq, width.cutoff = 500))
    message(formula_str)
  }

  # Helper function to run model
  run_model <- function(predictors) {
    eq <- stats::reformulate(c(predictors, "(1 | responder)"), response = "dfi_right_part")
    print_equation(eq)
    lme4::lmer(eq, data = data)
  }

  # Try to run the first model, if it fails, run the second
  model <- tryCatch(
    {
      run_model(predictors1)
    },
    error = function(e) {
      # If the first model fails, print the second model's equation
      eq2 <- stats::reformulate(c(predictors2, "(1 | responder)"), response = "dfi_right_part")
      print_equation(eq2)
      run_model(predictors2)
    }
  )

  return(model)
}
# run linear mixed model and merge model results
merge_lmm_results <- function(data, error_dfi, right_dfi_each_day) {
  # Prepare data and predictors
  all_name <- names(data)
  predictor_name1 <- setdiff(all_name, c("responder", "dfi_right_part", "seq_days"))
  predictor_name2 <- setdiff(predictor_name1, c("location", "lm_slope", "weight"))

  # Run the model
  mod <- run_lmm_models(data = data, predictors1 = predictor_name1, predictors2 = predictor_name2)

  #error_dfi <- test7$error_dfi
  test9 <- data.table::setDT(data)[, fitted := stats::predict(mod)]
  error_dfi <- merge(test9, error_dfi, all.x = TRUE, by = c("responder", "seq_days"))
  temp16 <- error_dfi[,c("responder", "seq_days", "fitted")]
  data.table::setnames(temp16, "fitted", "dfi_right_part")
  temp17 <- data.table::rbindlist(list(temp16, right_dfi_each_day))[order(responder, seq_days)]
  return(temp17)
}
# calculate adfi results
calculate_adfi <- function(data, adg_info, adg_data, feedintake_order, origin_dfi) {
  # Extract unique base information from feedintake_order
  base_info_origin = unique(feedintake_order[, .(responder, location, date, seq_days)])

  # Check if 'stage' column exists in adg_data
  has_stage <- "stage" %in% colnames(adg_data)

  # Select base_info based on the presence of 'stage' column
  base_info <- if (has_stage) {
    unique(adg_data[, .(responder, location, stage, date, seq_days)])
  } else {
    base_info_origin
  }

  # Merge data with base_info
  temp <- merge(data, base_info, all = FALSE)

  # Rename 'dfi_right_part' column to 'corrected_dfi'
  data.table::setnames(temp, "dfi_right_part", "corrected_dfi")

  # Merge temp18 with origin_dfi
  temp2 <- merge(temp, origin_dfi, all = FALSE)

  # Select columns based on the presence of 'stage' column
  temp3 <- if (has_stage) {
    temp2[, .(responder, location, stage, date, seq_days, origin_dfi, corrected_dfi)]
  } else {
    temp2[, .(responder, location, date, seq_days, origin_dfi, corrected_dfi)]
  }

  # Calculate mean of 'origin_dfi' and 'corrected_dfi' for each responder and location
  fin1 <- temp3[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("origin_dfi", "corrected_dfi"), by = .(responder, location)]

  # Calculate test duration for each responder and location
  fin2 <- temp3[, .(test_days = max(seq_days) - min(seq_days)), by = .(responder, location)]

  # Merge test duration with mean DFI information
  fin3 <- merge(fin2, fin1, all.x = T)

  # Create a list containing summary information and detailed data
  fin <- list(adfi_info = fin3, adfi_data = temp3)

  return(fin)
}
