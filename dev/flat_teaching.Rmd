---
title: "flat_teaching.Rmd for working package"
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
Run this 'development' chunk

Store every call to library() that you need to run chunks line by line, as in a classical Rmd for analysis
-->

```{r development, include=FALSE}
library(testthat)
```

<!--
# Description of your package

This will fill the description of your package.
Fill and run the content of this chunk, before anything else. 

Note: when you will use other flat templates, this part will be in a separate file. Do not be surprised!
--> 

```{r description, eval=FALSE}
# Describe your package
fusen::fill_description(
  pkg = here::here(),
  fields = list(
    Title = "Breeding Tools",
    Description = "An R package that can analyze the csv data from the Nedap in pig farm.",
    `Authors@R` = c(
      person("Guo Meng", email = "tony2015116@163.com", role = c("aut", "cre")),
      person(given = "Guo Meng", role = "cph")
    )
  ), 
  overwrite = TRUE
)
# Define License with use_*_license()
usethis::use_mit_license("Guo Meng")
```


# import_csv

The `import_csv()` function allows efficient importing of multiple CSV files into a single R data object. It takes a vector of file paths as the main argument, with an optional parameter to specify the package to use for reading CSVs. Valid options are "data.table", "vroom", and "readr". By default, data.table::fread is used for fast parsing. The function handles checking for valid input, loading the required package, looping through the files, reading each one with the appropriate CSV reader, and combining the results into a data.table or data.frame depending on the package used. This provides a convenient wrapper for batch importing CSVs without needing to write a for loop each time. To use it, supply a character vector of CSV file paths to the file_list argument. An optional second argument can choose the CSV reading package, with "data.table" being the default. The result is a consolidated data object containing the contents of all CSVs. This simplifies workflows that involve wrangling multiple CSV files into an analysis-ready dataset in R. By abstracting away the loop boilerplate, it allows the user to focus on the analytical tasks instead of data import details.

    
```{r function-import_csv}
#' Import CSV files using specified R packages
#'
#' @param file_list A list of file paths to CSV files
#' @param package Package to use for reading CSV. Options are "data.table", "vroom", or "readr". Default is "data.table"
#' @param ... Additional arguments to pass to the read function
#' @return A combined data.frame/data.table of all input CSV files
#' @export

import_csv <- function (file_list, package = "data.table", ...) {
  # Argument check
  if (!is.character(file_list) || !all(file.exists(file_list))) { # 检查file_list是否为字符向量并且所有的文件都存在
    stop("file_list must be a vector of existing file paths.")    # 如果检查失败，停止并返回错误消息
  }

  if (!is.character(package) || !package %in% c("data.table", "vroom", "readr")) { # 检查package参数是否为字符且属于指定的几个包之一
    stop("package must be one of 'data.table', 'vroom', 'readr'.") # 如果检查失败，停止并返回错误消息
  }

  # Load package
  if(!require(package, character.only = TRUE)) { # 尝试加载指定的包，如果加载失败，停止并返回错误消息
    stop(paste0("Package '", package, "' could not be loaded. Please install it."))
  }

  switch(package, # 根据package参数选择执行哪个分支的代码
         "data.table" = { # 如果package为"data.table"，执行以下代码
           data.table::rbindlist(purrr::map(file_list, function(x,
                                                                ...) {
             data.table::fread(x, ...)
           }, ...))
           # 使用purrr的map函数对file_list中的每个文件应用fread函数，然后使用rbindlist函数将结果合并为一个data.table
         },
         "vroom" = { # 如果package为"vroom"，执行以下代码
           vroom::vroom(file_list, ...) # 使用vroom函数直接读取file_list中的所有文件
         },
         "readr" = { # 如果package为"readr"，执行以下代码
           purrr::map_dfr(file_list, function(x, ...) {
             readr::read_csv(x, ...)
           }, ...)
           # 使用purrr的map_dfr函数对file_list中的每个文件应用read_csv函数，将结果合并为一个data.frame
         },
         stop("Invalid package specified.") # 如果package不属于以上任何一个，停止并返回错误消息
  )
}
```
  
```{r example-import_csv}
list_csv <- list.files("C:/Users/Dell/Downloads/location301", pattern = ".csv", full.names = TRUE)
data <- import_csv(list_csv)
```
  

# adg_get

The `adg_get()` function is used to process raw pig feed intake data to calculate average daily gain (ADG) and generate growth curves. It takes the raw data frame as input, cleans invalid records in multiple steps, runs robust regression to detect outliers, fits simple linear models to estimate ADG over desired weight ranges, and produces growth curve plots. The user can specify a weight range to segment the ADG calculation via the my_break parameter. Outlier detection threshold and output image save path can also be customized. After processing, it returns ADG summary info and the analyzed dataset with outlier flags. To use it, supply the raw data and optionally set parameters like my_break. It handles data cleaning, analysis and plotting automatically. The output contains ADG results that can be used for downstream modeling or evaluation. This provides a streamlined workflow to go from raw feed intake data to analyzed growth curves and ADG estimates in a single function call.

```{r function-adg_get}
#' ADG get from pig performance test station csv data
#' 
#' @param data A data frame or data table containing the nedap or fire pig performance test data to be processed. Columns must include 'visit_time', 'location', 'responder', 'feed_intake'.
#' @param my_break Optional, a numeric vector of length 2, indicates target weight range for calculating ADG, default NULL. If not NULL, ADG will be calculated within this range.
#' @param range_offset Optional, a numeric value, default 0.5. 
#' Used to extend the target weight range specified in my_break to avoid border effect.
#' For example, if my_break is c(60,90) and range_offset is 0.5, the actual range for analysis will be 57.5~92.5.
#' @param threshold Optional, a numeric value, default 1, used as the threshold to identify outliers in RANSAC regression, usually 0~2.
#' @param save_path Optional, a character string specifying where to save the generated growth curve images. If not NULL, images will be saved to this path.
#' @return A list containing:
#' - adg_info: A data.table containing ADG statistics
#' - adg_data: A data.table containing processed sample data
#' @importFrom stats "coef" "predict" "residuals" "sd"
#' @importFrom data.table ":=" ".SD" ".GRP" ".N"
#' 
#' @export

# get adg results and create plots
adg_get <- function(data, my_break = NULL, range_offset = 0.5, threshold = 1, save_path = NULL) {
  # parameters check
  validate_arguments(threshold, my_break, range_offset)
  
  # get adg results
  adg_results <- process_adg_data(data) |> 
    clean_and_filter_weight_data() |> 
    remove_outliers_using_ransac(threshold) |> 
    validate_responder_weights() |> 
    calculate_adg_and_clean_data(my_break = my_break , range_offset)
  
  # save adg plots
  if (!is.null(save_path)) {
    create_adg_plots <- create_adg_plots(data = adg_results$adg_data, my_break = my_break)
    message(crayon::cyan("\u2022 Printing growth curve images.Please wait a moment!"))
    save_adg_plots(data = create_adg_plots, my_break = my_break, save_path = save_path)
  }
  
  return(adg_results)
}
# process adg data
process_adg_data <- function(data) {
  if (missing(data)) stop("Missing data frame or data table!")

  # Ensure the input is a data.table and create a deep copy
  if (!data.table::is.data.table(data)) {
    data <- data.table::as.data.table(data, keep.rownames = FALSE)
  } else {
    data <- data.table::copy(data)
  }

  # Define and check required columns
  nedap_cols <- c("animal_number", "lifenumber", "responder", "location", "visit_time", "duration", "state", "weight", "feed_intake")
  missing_columns <- setdiff(nedap_cols, names(data))
  if (length(missing_columns) > 0) stop(paste("Missing columns:", paste(missing_columns, collapse = ", ")))

  # Convert columns to appropriate types
  data[, `:=`(
    location = as.character(location),
    responder = as.character(responder),
    visit_time = if (!inherits(visit_time, c("POSIXct", "POSIXlt"))) {
      warning("visit_time converted to POSIXct. Please check if the data source is consistent.")
      as.POSIXct(visit_time, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
    } else visit_time,
    date = as.Date(visit_time)
  )]

  # Remove duplicates and filter out rows where responder is NA
  data <- unique(data[!is.na(responder)])

  # Identify duplicate responders across locations
  unique_dt <- unique(data[, .(responder, location)])
  dup_responders <- unique_dt[, .N, by = .(responder)][N > 1]

  # Count records for each responder-location combination
  num_records <- unique(data[, `:=`(n, .N), .(responder, location)][, .(responder, location, n)])

  # Set 'responder' as the key for join operations
  data.table::setkey(dup_responders, responder)
  data.table::setkey(num_records, responder)

  # Perform left join operation on 'num_records' and 'dup_responders'
  dup_records <- num_records[dup_responders]

  # Print duplicate 'responder's and their 'location's
  if(nrow(dup_responders) > 0) {
    message(crayon::red("\u2022 There are", nrow(dup_responders), "duplicated responders."))
    print(dup_records)
  } else {
    message(crayon::cyan("\u2022 There are no duplicate responders in different locations."))
  }

  # Handle duplicate responders by assigning them to the location with most records
  if (nrow(dup_responders) > 0) {
    max_n_location <- num_records[, .(max_n = max(n), location_maxn = location[which.max(n)]), by = responder]
    max_n_location <- unique(max_n_location)
    data <- merge(data, max_n_location, by = "responder", all.x = TRUE)[, `:=`(location, location_maxn)][, `:=`(c("max_n", "location_maxn"), NULL)]
  }

  # Create the final dataset with additional columns
  # Create the final dataset with additional columns
  data_pre <- data[data.table::CJ(date = tidyr::full_seq(date, 1)), on = .(date)
  ][, `:=`(seq_days, .GRP), by = .(date)
  ][order(visit_time), `:=`(seq_in_day, 1:.N), by = .(responder, date)
  ][order(visit_time), `:=`(seq_in_location, 1:.N), by = .(location, date)
  ][order(responder, visit_time)][]

  return(data_pre)
}
# remove weight < 15kg , test date range < 35days and records < 20
clean_and_filter_weight_data <- function(data) {
  # Set key for faster operations on responder
  data.table::setkey(data, responder)

  # Calculate the maximum weight for each responder
  max_weights <- data[, .(max_weight = max(weight, na.rm = TRUE)), by = responder]

  # Find responders whose all weights are less than 15000
  responder_to_delete <- max_weights[max_weight < 15000, responder]

  # Print the number of responders to be deleted due to low weight
  if(length(responder_to_delete) > 0){
    message(crayon::red("\u2022 Removing weight < 15kg records will delete responders: ", length(responder_to_delete)))
  } else {
    message(crayon::cyan("\u2022 The removing of weight < 15kg will not delete responder."))
  }

  # Filter and process data
  temp <- data[!responder %in% responder_to_delete,
               `:=`(
                 n = .N,  # Count of records for each responder and location
                 date_na = sum(is.na(weight)),  # Count of NA weights
                 date_length = as.integer(difftime(max(date), min(date), units = "days"))  # Date range in days
               ),
               by = .(responder, location)
  ][, `:=`(
    test_days_less_than_40 = date_length < 35,  # Test if date range is less than 35 days
    test_records_less_than_20 = n < 20,  # Test if number of records is less than 20
    data_na_greater_than_one_third = ifelse(date_length == 0, FALSE, date_na/date_length >= 1/3)  # Test if more than 1/3 of data is NA
  )]

  # Calculate row_sum and find outliers
  temp[, row_sum := test_days_less_than_40 + test_records_less_than_20 + data_na_greater_than_one_third]
  outlier <- unique(temp[row_sum > 0, .(responder, location, date_na, test_days_less_than_40, test_records_less_than_20, data_na_greater_than_one_third, row_sum)])

  # Final result: keep rows with row_sum == 0 and weight >= 15000
  step1_res <- temp[row_sum == 0 & weight >= 15000, .(responder, location, date, seq_in_location, seq_days, seq_in_day, weight)]

  # Print information about outliers
  if(nrow(outlier) > 0){
    message(crayon::red("\u2022 Removing records of missing will delete responders:", nrow(outlier)))
  } else {
    message(crayon::cyan("\u2022 No responder is deleted due to missing records."))
  }

  # Combine all deleted responders and print
  deleted_responders <- c(responder_to_delete, outlier$responder)
  message(crayon::red('\u2022 Deleted responders: \n', paste0('c("', paste(deleted_responders, collapse = '","'), '")')))

  # Check if the processed data is empty
  if (nrow(step1_res) == 0) {
    stop("The processed data is empty!")
  }

  return(step1_res)
}
# run robust regression model
process_lmrob_results <- function(data, threshold, ...) {
  # Safely apply lmrob function
  safelmrob <- purrr::safely(.f = robustbase::lmrob)

  # Process data using data.table syntax for efficiency
  temp1 <- data[, `:=`(safe_lmrob = purrr::map(data, \(df, ...) safelmrob(..., data = df), ...))
  ][, `:=`(
    model_lmrob = purrr::map(safe_lmrob, "result"),
    error_msg = purrr::map_chr(safe_lmrob, \(x) if (is.null(x$error)) "" else x$error$message),
    warning_msg = purrr::map_chr(safe_lmrob, \(x) if (is.null(x$warning)) "" else x$warning$message)
  )][, `:=`(
    residuals = purrr::pmap(list(model_lmrob, error_msg, warning_msg),
                            \(x, e, w) if (e == "" && w == "") {return(residuals(x))} else {return(NULL)}),
    predict = purrr::pmap(list(model_lmrob, error_msg, warning_msg),
                          \(x, e, w) if (e == "" && w == "") {return(predict(x))} else {return(NULL)}))
  ][, `:=`(mean_residual = purrr::map_dbl(residuals, \(r) mean(r, na.rm = TRUE)),
           sd_residual = purrr::map_dbl(residuals, \(r) sd(r, na.rm = TRUE)))
  ][, `:=`(outliers = purrr::pmap(list(residuals, mean_residual, sd_residual),\(r, m, sd) if (length(r) > 0) abs(r - m) > threshold * sd else NULL))][]


  # Clean up temporary columns and expand data
  temp2 <- temp1[, c("safe_lmrob", "model_lmrob", "error_msg", "warning_msg",
                     "residuals", "mean_residual", "sd_residual") := NULL][, {
                       dt <- data.table::as.data.table(data[[1]])
                       dt[, `:=`(
                         predict = unlist(predict),
                         outliers = unlist(outliers))]
                       dt}, by = responder]

  return(temp2)
}
# set robust regression
remove_outliers_using_ransac <- function(data, threshold, tuning.chi = 1.548, k.max = 1000, maxit.scale = 1000, max.it = 1000) {
  # Count initial responders
  begin_responder <- unique(data[, .(responder)])

  # Generate nested data format
  data <- data[, .(data = list(.SD)), by = responder]

  # Set RANSAC parameters
  control_params <- robustbase::lmrob.control(tuning.chi = tuning.chi, k.max = k.max, maxit.scale = maxit.scale, max.it = max.it)

  #Print message and check for errors and warnings
  message(crayon::cyan("\u2022 Running RANSAC Robust Regression:"))

  # Process data using RANSAC
  tryCatch({
    lm_results <- process_lmrob_results(data = data, threshold = threshold,
                                        weight ~ seq_days + I(seq_days^2),
                                        control = control_params)
    message(crayon::cyan("\u2022 RANSAC Robust Regression succeeded!"))

    # Count remaining responders after outlier removal
    end_responder <- unique(lm_results[outliers == FALSE, .(responder)])
    responder_to_delete <- begin_responder[!end_responder, on = "responder"]
    deleted_responders <- responder_to_delete$responder

    # Print information about deleted responders
    if (nrow(responder_to_delete) > 0) {
      message(crayon::red(paste0("\u2022 Detecting outliers using model will delete responders: ",
                                 nrow(responder_to_delete))))
      message(crayon::red(paste0('\u2022 Deleted responders: \n',
                                 'c("', paste(deleted_responders, collapse = '","'), '")')))
    } else {
      message(crayon::cyan("\u2022 The outliers detected by Robust model will not delete responder."))
    }

    # Remove outliers and affected responders
    lm_results[!responder %in% responder_to_delete$responder]
  }, error = function(e) {
    stop(crayon::red(paste("Error:", conditionMessage(e))))
  })
}
# get the maximum/minimum weight
get_extreme_weights <- function(data, seq_days, direction, weight_type) {
  data[, keyby = .(responder), `:=`(temp, data.table::frankv(direction * seq_days, ties.method = "dense") <= 2)]
  filtered_data <- data[temp == TRUE, keyby = .(responder, location), .(temp_weight = stats::median(predict))]
  data.table::setnames(filtered_data, "temp_weight", weight_type)
  return(filtered_data)
}
# check minimum entry weight and maximum exit wieght 
validate_responder_weights <- function(data, entry_weight_limit = 60, exit_weight_limit = 85) {
  # Filter out outliers
  filtered_data <- data[outliers == FALSE]

  # Get minimum weight
  min_weights <- get_extreme_weights(filtered_data, seq_days, 1, "min_weight")

  # Identify responders with entry weight exceeding the limit
  overweight_responders <- unique(min_weights[min_weight > (entry_weight_limit * 1000), .(responder)])
  num_overweight <- nrow(overweight_responders)

  # Log overweight responders
  if (num_overweight > 0) {
    message(crayon::red(paste0("\u2022 Removing begin_test_weight >", entry_weight_limit,
                               "kg records will delete responders:", num_overweight)))
  } else {
    message(crayon::cyan(paste0("\u2022 All responders' begin_test_weight are less than or equal to ",
                                 entry_weight_limit, "kg.")))
  }

  # Remove overweight responders
  data_filtered <- data[!responder %in% overweight_responders$responder]

  # Get maximum weight
  max_weights <- get_extreme_weights(data_filtered, seq_days, -1, "max_weight")

  # Identify responders with exit weight below the limit
  underweight_responders <- unique(max_weights[max_weight < (exit_weight_limit * 1000), .(responder)])
  num_underweight <- nrow(underweight_responders)

  # Log underweight responders
  if (num_underweight > 0) {
    message(crayon::red(paste0("\u2022 Removing end_test_weight <", exit_weight_limit, "kg records will delete responders: ", num_underweight)))
    deleted_responders <- c(overweight_responders$responder, underweight_responders$responder)
    message(crayon::red('\u2022 Deleted responders: \n', paste0('c("', paste(deleted_responders, collapse = '","'), '")')))
    #message(crayon::red('\u2022 Deleted responders: \n', paste0('c("', paste(deleted_responders, collapse = '","'), '")')))
  } else {
    message(crayon::cyan(paste0("\u2022 All responders' end_test_weight are more than or equal to ",
                                 exit_weight_limit, "kg.")))
  }

  # Return data for responders who passed weight criteria
  passed_responders <- data_filtered[!responder %in% underweight_responders$responder]
  return(passed_responders[, !c("temp")])
}
# run simple linear regression model
process_lm_results <- function(data, ...) {
  safelm = purrr::safely(.f = stats::lm)
  temp1 <- data[outliers == FALSE]
  temp2 <- temp1[, .(data = list(.SD)), by = responder
  ][, `:=`(safe_lm, purrr::map(data, \(df, ...) safelm(..., data = df), ...))
  ][, `:=`(safe_lm = purrr::map(safe_lm, "result"),
           error_msg = purrr::map_chr(safe_lm, \(x) if (is.null(x$error)) "" else x$error$message),
           warning_msg = purrr::map_chr(safe_lm, \(x) if (is.null(x$warning)) "" else x$warning$message))
  ][, `:=`(lm_predict, purrr::map2(safe_lm, warning_msg, \(x, w) if (w == "") stats::predict(x) else NA))
  ][, `:=`(lm_slope, purrr::map(safe_lm, \(x) coef(x)["seq_days"]))
  ][, `:=`(r_squared, purrr::map_dbl(safe_lm, \(x) if (!is.null(x)) summary(x)$r.squared else NA))]

  final <- temp2[, c("responder", "lm_slope", "r_squared")]
  final[, lm_slope := unlist(lm_slope)]
  return(final)
}
# parameters check
validate_arguments <- function(threshold, my_break, range_offset) {
  # Validate input threshold
  if (!is.numeric(threshold) || length(threshold) != 1 || threshold < 0 || threshold > 2) {
    stop("Argument 'threshold' should be a single numeric value between 0 and 2")
  }

  if (!is.null(my_break) && (!is.numeric(my_break) || length(my_break) != 2 || any(my_break < 0))) {
    stop("Argument 'my_break' should be a numeric vector of length 2 with non-negative values or NULL")
  }
  if (!is.numeric(range_offset) || length(range_offset) != 1 || range_offset < 0 || range_offset > 1) {
    stop("Argument 'range_offset' should be a single numeric value in the range [0, 1]")
  }
}
# cut weight in range
cut_weight <- function(data, my_break, range_offset) {
  # Convert my_break to grams to match weight representation in data
  my_break <- my_break * 1000
  # Generate actual break points using the given range and offset
  actual_breaks <- c(my_break[1] - range_offset * 1000, my_break[2] + range_offset * 1000)
  # Select weights within the specified range
  data <- data[predict >= actual_breaks[1] & predict <= actual_breaks[2], ]
  # Add a 'stage' column to represent the selected weight range
  data[, `:=`(stage, paste0(my_break[1] / 1000, "-", my_break[2] / 1000))]
  return(data)
}
# set simple linear regression model
calculate_adg_and_clean_data <- function(data, my_break, range_offset) {
  # Create a deep copy of the input data
  data <- data.table::copy(data)

  # Print message
  message(crayon::cyan("\u2022 Running Simple Linear Regression"))

  # Main function execution wrapped in tryCatch for error handling
  final <- tryCatch({
    if (!is.null(my_break)) {
      # Process data with specified weight break
      cut_data <- cut_weight(data = data, my_break = my_break, range_offset = range_offset)
      slopes_cut <- process_lm_results(data = cut_data, weight ~ seq_days)
      cat(crayon::cyan(paste0("\u2022 Calculate ADG at ", my_break[1], "~", my_break[2], "kg weight range using Simple Linear Regression succeeded!\n")))

      # Merge processed data and calculate additional metrics
      final <- merge(cut_data, slopes_cut, by = "responder")[, .(responder, location, stage, date, seq_in_location, seq_days, seq_in_day, weight, predict, r_squared, lm_slope, outliers)]
      temp_weight_get <- final[outliers == FALSE]
      temp_min_weight <- get_extreme_weights(data = temp_weight_get, seq_days, 1, "min_weight_cut")
      temp_max_weight <- get_extreme_weights(data = temp_weight_get, seq_days, -1, "max_weight_cut")

      # Calculate additional information
      info_temp_date <- data.table::copy(final)[, .(start_date_cut = min(date), end_date_cut = max(date)), by = responder]
      info_temp_base <- unique(final[, .(responder, stage, r_squared, lm_slope)])
      info_temp <- data.table::merge.data.table(info_temp_base, info_temp_date)
      weight_temp <- data.table::merge.data.table(temp_min_weight, temp_max_weight)
      temp_final <- data.table::merge.data.table(info_temp, weight_temp)

      # Calculate stage days and prepare final output
      temp_final <- temp_final[, stage_days := (my_break[2] * 1000 - my_break[1] * 1000) / lm_slope]
      temp_final <- temp_final[, .(responder, location, stage, start_date_cut, min_weight_cut, end_date_cut, max_weight_cut, r_squared, lm_slope, stage_days)]
      fin <- list(adg_info = temp_final, adg_data = final)
    } else {
      # Process all data without weight break
      slopes <- process_lm_results(data = data, weight ~ seq_days)
      message(crayon::cyan("\u2022 Calculate ADG using Simple Linear Regression succeeded!"))

      # Merge processed data and calculate additional metrics
      final <- merge(data, slopes, by = "responder")
      temp_weight_get <- final[outliers == FALSE]
      temp_min_weight <- get_extreme_weights(data = temp_weight_get, seq_days, 1, "min_weight_origin")
      temp_max_weight <- get_extreme_weights(data = temp_weight_get, seq_days, -1, "max_weight_origin")

      # Calculate additional information
      info_temp_date <- data.table::copy(final)[, .(start_date_origin = min(date), end_date_origin = max(date)), by = responder]
      info_temp_base <- unique(final[, .(responder, r_squared, lm_slope)])
      info_temp <- data.table::merge.data.table(info_temp_base, info_temp_date)
      weight_temp <- data.table::merge.data.table(temp_min_weight, temp_max_weight)
      temp_final <- data.table::merge.data.table(info_temp, weight_temp)

      # Prepare final output
      temp_final <- temp_final[, .(responder, location, start_date_origin, min_weight_origin, end_date_origin, max_weight_origin, r_squared, lm_slope)]
      fin <- list(adg_info = temp_final, adg_data = final)
    }
  }, error = function(e) {
    # Error handling
    message(crayon::red("\u2022 Error: ", conditionMessage(e)))
    stop("Execution error, the program has stopped")
  })

  return(final)
}
# create adg plots
create_adg_plots <- function(data, my_break) {
  # Prepare data: add color_judge column and group by location
  data <- data[, `:=`(color_judge, data.table::fifelse(outliers == F, "Normal", "Outlier"))
  ][, .(data = list(.SD)), location]

  # Create plots for each location
  data[, `:=`(plot, purrr::map2(data, location, function(.x, .y) {
    # Calculate slopes and R-squared values for each responder
    slopes_and_r_squared <- .x[, .(lm_slope = unique(lm_slope), r_squared = unique(r_squared)), by = responder]

    # Prepare text labels based on whether my_break is provided
    if (!is.null(my_break)) {
      slopes_and_r_squared[, day_diff := (my_break[2] * 1000 - my_break[1] * 1000) / lm_slope]
      slopes_and_r_squared[, day_text := sprintf("Slope: %.2f, R^2: %.2f\n%d~%d kg: %.1f days", lm_slope, r_squared, my_break[1], my_break[2], day_diff)]
    } else {
      slopes_and_r_squared[, day_text := sprintf("Slope: %.2f, R^2: %.2f", lm_slope, r_squared)]
    }

    # Create ggplot object
    ggplot2::ggplot(data = .x, ggplot2::aes(x = date, y = weight)) +
      # Set theme
      ggplot2::theme_bw() +
      # Add points for each data point, colored by outlier status
      ggplot2::geom_point(ggplot2::aes(col = color_judge), size = 1, na.rm = T) +
      # Set color scale for normal and outlier points
      ggplot2::scale_color_manual(values = c(Normal = "#38b48b", Outlier = "#b81a35"), name = "robust regression") +
      # Format x-axis (date)
      ggplot2::scale_x_date(date_breaks = "2 day", date_labels = "%m-%d") +
      # Add prediction line
      ggplot2::geom_line(ggplot2::aes(x = date, y = predict), na.rm = T) +
      # Create separate plots for each responder
      ggplot2::facet_wrap( ~ as.numeric(responder), ncol = 2) +
      # Set y-axis scale
      ggplot2::scale_y_continuous(breaks = seq(15000, 130000, 15000), limits = c(15000, 130000)) +
      # Add title
      ggplot2::labs(title = paste("Location:", .y)) +
      # Customize theme elements
      ggplot2::theme(legend.position = "bottom",
                     legend.title = ggplot2::element_text(size = 20),
                     legend.text = ggplot2::element_text(size = 20),
                     axis.text.x = ggplot2::element_text(angle = -90, size = 10),
                     plot.title = ggplot2::element_text(size = 25, face = "bold")
      ) +
      # Add horizontal reference lines
      ggplot2::geom_hline(yintercept = 30000, linetype = "dashed", color = "#aed0ee") +
      ggplot2::geom_hline(yintercept = 60000, linetype = "dashed", color = "#aed0ee") +
      ggplot2::geom_hline(yintercept = 115000, linetype = "dashed", color = "#aed0ee") +
      #ggplot2::geom_hline(yintercept = 120000, linetype = "dashed", color = "#aed0ee") +
      # Add text annotations for slope and R-squared
      ggplot2::geom_text(data = slopes_and_r_squared, mapping = ggplot2::aes(label = day_text, x = min(.x$date), y = 130000, group = responder), hjust = 0, vjust = 1, size = 3)
  }))]

  # Return the data with added plots
  return(data)
}
# save plots
save_adg_plots <- function(data, my_break, save_path) {
  # Add argument checks
  if (!is.character(save_path) || length(save_path) != 1) {
    stop("Argument 'save_path' should be a single character string")
  }
  if (!dir.exists(save_path)) {
    stop(paste0("Directory '", save_path, "' does not exist"))
  }

  # Calculate the number of responders and days for each location
  location_dims <- data[, .(n_responders = data.table::uniqueN(data[[1]]$responder),
                            n_days = data.table::uniqueN(data[[1]]$date)), by = location]

  # Adjust the width and height of the images
  adjusted_dims <- location_dims[, .(width = 0.5 * n_days, height = 5 * n_responders)]

  # Generate filenames for saving plots
  filename <- if (!is.null(my_break)) {
    # If my_break is provided, include weight range in the filename
    weight_range <- paste0(my_break[1], "-", my_break[2])
    file.path(save_path, paste0("location_", data$location, "_", weight_range, "_growth.png"))
  } else {
    # If my_break is not provided, use a simpler filename
    file.path(save_path, paste0("location_", data$location, "_growth.png"))
  }

  # Save the plots
  purrr::walk2(filename, data$plot, function(file, plot, width, height) {
    ggplot2::ggsave(filename = file, plot = plot, width = width, height = height, units = "cm", dpi = "retina")
  }, width = adjusted_dims$width, height = adjusted_dims$height)
}
```

<!--
Here is an example on how to use the function.
This should be a reproducible and working example
-->

```{r examples-adg_get}
nedap_csv_data <- mintyr::nedap
adg_results <- adg_get(data = nedap_csv_data)
head(adg_results$adg_info)
```


# adfi_get

The `adfi_get()` function processes raw pig feed intake visit data to correct outliers and errors and calculate daily feed intake (DFI). It takes the raw data and outputs from `adg_get()` as input. It handles data cleaning by flagging invalid visit data based on predefined rules. Errors are corrected by redistributing feed intake to valid visits. Final DFI is calculated after removing errors. The user can optionally specify a weight range to match the DFI calculation to segmented ADG. Key outputs are DFI summary info and the analyzed dataset. To use it, supply the raw data and `adg_get()` outputs. It handles error detection, correction, DFI calculation and matching to ADG segments automatically. The output contains corrected DFI metrics ready for evaluation. This provides an automated workflow to go from raw feed intake visit data to analyzed DFI estimates matched to ADG weight segments in a single function.
    
```{r function-adfi_get}
#' ADFI get from pig performance test station csv data
#' 
#' @param data A data frame or data table containing the nedap or fire pig performance test data to be processed. Columns must include 'visit_time', 'location', 'responder', 'feed_intake'.
#' @param adg_res ADG results from function of adg_get().
#'
#' @return A list containing:
#' - dfi_info: A data.table containing DFI statistics
#' - dfi_data: A data.table containing processed sample data
#' 
#' @importFrom data.table ":=" ".SD" ".GRP" ".N"
#' 
#' @export

# get adfi results
adfi_get <- function(data, adg_res) {
  # prepare dfi data
  prepare_dfi_data <- process_adg_data(data) |>
    process_dfi_data()
  
  # prepare about error types
  prepare_error_types <- error_prop_get(prepare_dfi_data)
  
  # prepare scaled data
  scale_data <- prepare_scale_data(right_dfi = prepare_error_types$right_dfi, error_prop = prepare_error_types$error_prop_result, adg_info = adg_res$adg_info, adg_data = adg_res$adg_data)
  
  # run model and merge model predict results
  merge_model_results <- merge_lmm_results(data = scale_data$processed_data, error_dfi = prepare_error_types$error_dfi, right_dfi_each_day = scale_data$right_dfi_each_day)
  
  # get adfi results
  adfi_results <- calculate_adfi(merge_model_results, adg_info = adg_res$adg_info, adg_data = adg_res$adg_data, feedintake_order = prepare_dfi_data$feedintake_order, origin_dfi = prepare_error_types$origin_dfi)

  return(adfi_results)
}
# dfi data transform
process_dfi_data <- function(data) {
  # Make a copy of the input data to preserve the original dataset.
  data_pre <- data.table::copy(data)

  # Select specific columns from the data table.
  data_pre[, .(responder, location, date, seq_in_location, seq_days, seq_in_day, duration, visit_time, feed_intake, weight)]

  # Split the dataset by 'location' into a list of data tables.
  data_pre2 = split(data_pre, by = "location")

  # Apply a function to each element of the list to process and rename columns.
  data_pre3 = purrr::map(data_pre2, \(x) {
    temp = x[order(visit_time)][responder != 0 & !is.na(location)]
    data.table::setnames(temp, c("feed_intake", "duration", "visit_time"),
                         c("fiv", "otv", "entrancetime"))
    temp[, `:=`(entrancefeedweight = 0, exitfeedweight = 0,
                exittime = entrancetime + as.integer(otv), #lubridate::seconds(otv)
                frv = fiv/(otv/60))][, `:=`(lwd = 0, fwd = 0,
                                            ltd = 0, ftd = 0)]
  })

  # Print messages indicating successful generation of variables.
  message(crayon::cyan("\u2022 Successfully generated the following 3 variables:"))
  message(crayon::green(" - FIV:feed intake per visit;"))
  message(crayon::green(" - OTV:occupation time per visit;"))
  message(crayon::green(" - FRV:feeding rate per visit;"))
  # More variables are commented out for future use or debugging.

  # Further processing to identify outlier and erroneous data entries.
  transformed_1 <- purrr::map(data_pre3, \(x) {
    x[, `:=`(
      fiv_lo = data.table::fifelse(fiv < -20, 1, 0),
      fiv_hi = data.table::fifelse(fiv > 2000, 1, 0),
      fiv_0 = data.table::fifelse(otv == 0 & abs(fiv) > 20, 1, 0),
      otv_lo = data.table::fifelse(otv < 0, 1, 0),
      otv_hi = data.table::fifelse(otv > 3600, 1, 0),
      frv_hi_fiv_lo = data.table::fifelse(fiv > 0 & fiv < 50 & frv > 500, 1, 0),
      frv_hi_strict = data.table::fifelse(fiv >= 50 & any(data.table::shift(fiv, type = "lag") < -20, data.table::shift(fiv, type = "lead") < -20) & frv > 110, 1, 0),
      frv_hi = data.table::fifelse(fiv >= 50 & any(data.table::shift(fiv, type = "lag", n = 2) < -20, data.table::shift(fiv, type = "lead", n = 2) < -20) & frv > 170, 1, 0),
      frv_0 = data.table::fifelse(frv == 0 & otv > 500, 1, 0),
      frv_lo = data.table::fifelse(frv != 0 & abs(frv) <= 2, 1, 0),
      lwd_lo = data.table::fifelse((!is.na(lwd)) & lwd < -20, 1, 0),
      lwd_hi = data.table::fifelse((!is.na(lwd)) & lwd > 1800, 1, 0),
      fwd_lo = data.table::fifelse((!is.na(fwd)) & fwd < -20, 1, 0),
      fwd_hi = data.table::fifelse((!is.na(fwd)) & fwd > 1800, 1, 0),
      ltd_lo = data.table::fifelse((!is.na(ltd)) & ltd < 0, 1, 0),
      ftd_lo = data.table::fifelse((!is.na(ftd)) & ftd < 0, 1, 0)
    )]
  })
  transformed_2 = data.table::rbindlist(transformed_1, use.names = T, fill = T)

  # Print a message indicating the number of error types generated from variables.
  message(crayon::green("\u2022 Successfully generated 10 error types from 3 variables:"))
  message(crayon::green(" - FIV-lo; FIV-hi; FIV-0; OTV-lo; OTV-hi; FRV-hi-FIV-lo; FRV-hi-strict; FRV-hi; FRV-0; FRV-lo;"))
  # Additional error types are commented out.

  # Aggregate and organize the final transformed data for further analysis.
  transformed_3 <- transformed_2[, .(date, seq_in_day, seq_days, location, responder, entrancetime, exittime, entrancefeedweight, exitfeedweight, weight, otv, fiv, frv, ltd, ftd, lwd, fwd, fiv_lo, fiv_hi, fiv_0, otv_lo, otv_hi, frv_hi_fiv_lo, frv_hi_strict, frv_hi, frv_0, frv_lo, lwd_lo, lwd_hi, fwd_lo, fwd_hi, ltd_lo, ftd_lo)]

  # Extract feed intake data with order.
  extract_feed_intake_order <- transformed_2[, .(responder, location, date, seq_in_location, seq_days, seq_in_day, fiv, weight)]

  # Prepare final data as a list containing transformed data and feed intake order.
  final <- list(transformed_data = transformed_3, feedintake_order = extract_feed_intake_order)

  # Return the final list containing all processed data.
  return(final)
}
# create error types and error types' prop
error_prop_get <- function(data) {
  # Define error types
  ERROR_TYPES <- c("fiv_lo", "fiv_hi", "fiv_0", "otv_lo", "otv_hi", "frv_hi_fiv_lo", "frv_hi_strict", "frv_hi", "frv_0",
                   "frv_lo", "lwd_lo", "lwd_hi", "fwd_lo", "fwd_hi", "ltd_lo", "ftd_lo")

  # Select required columns and calculate total errors (OE)
  processed_data <- data$transformed_data[, c("date", "seq_in_day", "seq_days", "location", "responder", "weight",
                                              "otv", "fiv", "frv", "ltd", "ftd", "lwd", "fwd",
                                              ..ERROR_TYPES)]
  processed_data[, OE := rowSums(.SD, na.rm = TRUE), .SDcols = ERROR_TYPES]

  # Calculate original Daily Feed Intake (DFI)
  origin_dfi <- processed_data[, .(origin_dfi = sum(fiv)), by = .(responder, seq_days)]

  # Calculate correct DFI part
  right_dfi <- processed_data[OE == 0, .(dfi_right_part = sum(fiv)), by = .(responder, seq_days)]

  # Extract error DFI data
  error_dfi_data <- processed_data[OE != 0]
  error_dfi <- error_dfi_data[, .(dfi_error_part = sum(fiv)), by = .(responder, seq_days)]

  # Function to transform OTD and FID data
  otd_fid_trans <- function(data, target_col, error_cols, new_names, ...) {
    # Inner function to sum target column for rows where error column > 0
    temp1 <- function(col_names, ...) {
      eval(as.name(data))[eval(as.name(col_names)) > 0, by = .(responder, seq_days), purrr::map(.SD, sum), .SDcols = target_col]
    }

    # Apply temp1 function to each error column
    temp2 <- purrr::map(error_cols, temp1)

    # Rename the summed columns with new names
    temp3 <- purrr::map2(temp2, new_names, function(x, y) data.table::setnames(x, target_col, y))

    # Merge all the resulting data tables
    temp4 <- Reduce(function(x, y) merge(x, y, by = c("responder", "seq_days"), all = TRUE), temp3)

    return(temp4)
  }

  # Create a data table with parameters for OTD and FID transformations
  useful_list <- data.table::data.table(
    data = c("error_dfi_data", "error_dfi_data"),
    target_col = c("otv", "fiv"),
    error_cols = list(I(names(error_dfi_data)[c(14:15, 19:27)]), I(names(error_dfi_data)[c(17:18, 28:29)])),
    new_names = list(I(paste0("otd_", c(1:2, 6:14))), I(paste0("fid_", c(4:5, 15:16))))
  )

  # Apply the otd_fid_trans function to each row of useful_list
  otd_fid_result <- purrr::pmap(useful_list, otd_fid_trans)

  # Merge OTD and FID results
  merged_result <- Reduce(function(x, y) merge(x, y, by = c("responder", "seq_days"), all = TRUE), otd_fid_result)

  # Calculate error type proportions
  error_cols <- names(processed_data)[14:29]
  error_proportions <- processed_data[, c(.N, lapply(.SD, sum)), .SDcols = error_cols, by = .(responder, seq_days)]
  error_proportions[, (paste0(error_cols, "_p")) := lapply(.SD, function(x) x / N), .SDcols = error_cols, by = .(responder, seq_days)]
  error_proportions[, c(error_cols, "N") := NULL]

  # Merge all results
  final_result <- merge(merged_result, error_proportions, by = c("responder", "seq_days"), all.x = TRUE)
  final_result <- merge(final_result, error_dfi, by = c("responder", "seq_days"), all.x = TRUE)

  # Replace NA with 0
  numeric_cols <- names(final_result)[sapply(final_result, is.numeric)]
  data.table::setnafill(final_result, fill = 0, cols = numeric_cols)

  final <- list(error_prop_result = final_result, origin_dfi = origin_dfi, right_dfi = right_dfi, error_dfi = error_dfi)

  return(final)
}
# prepare scale data
prepare_scale_data <- function(right_dfi, error_prop, adg_info, adg_data) {
  # Extract right DFI data for days with errors
  right_dfi_in_one_day <- error_prop[right_dfi, on = .(responder, seq_days)][!is.na(dfi_error_part)][, dfi_error_part := NULL]

  # Extract right DFI data for days without errors
  right_dfi_each_day <- error_prop[right_dfi, on = .(responder, seq_days)][is.na(dfi_error_part)][, .(responder, seq_days, dfi_right_part)]

  # Extract base info for ADG (Average Daily Gain)
  base_info_adg <- adg_info[, .(responder, location, lm_slope)]

  # Extract and calculate average weight for each responder and day
  base_info_weight <- unique(adg_data[, weight := as.double(weight)][, .(responder, seq_days, weight)][, weight := mean(weight, na.rm = TRUE), by = .(responder, seq_days)][])

  # Merge DFI data with ADG info
  merged_data <- merge(right_dfi_in_one_day, base_info_adg, by = "responder", all.x = TRUE)

  # Merge with weight info
  merged_data <- merge(merged_data, base_info_weight, by = c("responder", "seq_days"), all.x = TRUE)

  # Remove rows with NA weight
  merged_data <- merged_data[!is.na(weight)]#[, .(LWD-lo, LWD-hi, FWD-lo, FWD-hi, LTD-lo, FTD-lo) := NULL]

  # Process data using recipes package
  processed_data <- data.table::setDF(merged_data) |>
    recipes::recipe(dfi_right_part ~ .) |>
    recipes::update_role(responder, seq_days, location, new_role = "id") |>
    recipes::step_zv(recipes::all_numeric()) |>
    recipes::step_corr(recipes::all_predictors(), threshold = 0.9) |>
    #recipes::step_zv(recipes::all_numeric()) |>
    recipes::step_scale(recipes::all_predictors()) |>
    recipes::prep() |>
    recipes::juice() #|>
    #dplyr::mutate_at("responder", as.factor)
  processed_data$responder <- as.factor(processed_data$responder)

  final <- list(processed_data = processed_data, right_dfi_each_day = right_dfi_each_day)

  return(final)
}
# create linear mixed model
run_lmm_models <- function(data, predictors1, predictors2) {
  # Helper function to format and print equation
  print_equation <- function(eq) {
    formula_str <- paste0(crayon::cyan("\u2022 Running linear mixed model with equation: \n"), " ", deparse(eq, width.cutoff = 500))
    message(formula_str)
  }

  # Helper function to run model
  run_model <- function(predictors) {
    eq <- stats::reformulate(c(predictors, "(1 | responder)"), response = "dfi_right_part")
    print_equation(eq)
    lme4::lmer(eq, data = data)
  }

  # Try to run the first model, if it fails, run the second
  model <- tryCatch(
    {
      run_model(predictors1)
    },
    error = function(e) {
      # If the first model fails, print the second model's equation
      eq2 <- stats::reformulate(c(predictors2, "(1 | responder)"), response = "dfi_right_part")
      print_equation(eq2)
      run_model(predictors2)
    }
  )

  return(model)
}
# run linear mixed model and merge model results
merge_lmm_results <- function(data, error_dfi, right_dfi_each_day) {
  # Prepare data and predictors
  all_name <- names(data)
  predictor_name1 <- setdiff(all_name, c("responder", "dfi_right_part", "seq_days"))
  predictor_name2 <- setdiff(predictor_name1, c("location", "lm_slope", "weight"))

  # Run the model
  mod <- run_lmm_models(data = data, predictors1 = predictor_name1, predictors2 = predictor_name2)

  #error_dfi <- test7$error_dfi
  test9 <- data.table::setDT(data)[, fitted := stats::predict(mod)]
  error_dfi <- merge(test9, error_dfi, all.x = TRUE, by = c("responder", "seq_days"))
  temp16 <- error_dfi[,c("responder", "seq_days", "fitted")]
  data.table::setnames(temp16, "fitted", "dfi_right_part")
  temp17 <- data.table::rbindlist(list(temp16, right_dfi_each_day))[order(responder, seq_days)]
  return(temp17)
}
# calculate adfi results
calculate_adfi <- function(data, adg_info, adg_data, feedintake_order, origin_dfi) {
  # Extract unique base information from feedintake_order
  base_info_origin = unique(feedintake_order[, .(responder, location, date, seq_days)])

  # Check if 'stage' column exists in adg_data
  has_stage <- "stage" %in% colnames(adg_data)

  # Select base_info based on the presence of 'stage' column
  base_info <- if (has_stage) {
    unique(adg_data[, .(responder, location, stage, date, seq_days)])
  } else {
    base_info_origin
  }

  # Merge data with base_info
  temp <- merge(data, base_info, all = FALSE)

  # Rename 'dfi_right_part' column to 'corrected_dfi'
  data.table::setnames(temp, "dfi_right_part", "corrected_dfi")

  # Merge temp18 with origin_dfi
  temp2 <- merge(temp, origin_dfi, all = FALSE)

  # Select columns based on the presence of 'stage' column
  temp3 <- if (has_stage) {
    temp2[, .(responder, location, stage, date, seq_days, origin_dfi, corrected_dfi)]
  } else {
    temp2[, .(responder, location, date, seq_days, origin_dfi, corrected_dfi)]
  }

  # Calculate mean of 'origin_dfi' and 'corrected_dfi' for each responder and location
  fin1 <- temp3[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("origin_dfi", "corrected_dfi"), by = .(responder, location)]

  # Calculate test duration for each responder and location
  fin2 <- temp3[, .(test_days = max(seq_days) - min(seq_days)), by = .(responder, location)]

  # Merge test duration with mean DFI information
  fin3 <- merge(fin2, fin1, all.x = T)

  # Create a list containing summary information and detailed data
  fin <- list(adfi_info = fin3, adfi_data = temp3)

  return(fin)
}
```
  
```{r example-adfi_get}
nedap_csv_data <- mintyr::nedap
adg_results <- adg_get(data = nedap_csv_data)
adfi_results <- adfi_get(data = nedap_csv_data, adg_res = adg_results)
head(adfi_results$adfi_info)
```
  
  
# fcr_get

The `fcr_get()` function computes feed conversion ratio (FCR) by combining average daily gain (ADG) and daily feed intake (DFI) data. It takes the output lists from `adg_get()` and `dfi_get()` as input, merges the ADG and DFI summary dataframes by pig ID and location, calculates FCR as corrected DFI divided by ADG slope, and optionally adjusts FCR for a specified weight range. The output is an FCR dataframe with pig ID, location, FCR, and adjusted FCR if weight range is provided. To use it, supply the `adg_get()` and `dfi_get()` result lists. It handles merging the data, FCR calculation, and adjustment automatically. This provides a streamlined workflow to go from raw feed intake data to analyzed FCR estimates matched to ADG weight segments. The output FCR metrics can then be used for evaluation and modeling of feed efficiency. By combining ADG and DFI data, `fcr_get()` enables easy computation of the key FCR feed efficiency metric.
    
```{r function-fcr_get}
#' Calculate Feed Conversion Ratio (FCR)
#'
#' This function calculates the Feed Conversion Ratio (FCR) based on average daily gain (ADG) and average daily feed intake (ADFI) results.
#' It also calculates a corrected FCR if stage information is available.
#'
#' @param adg_res A list containing ADG results, including 'adg_info' and 'adg_data' from adg_get().
#' @param adfi_res A list containing ADFI results, including 'adfi_info' and 'adfi_data' from adfi_get().
#'
#' @return A list containing:
#'   \item{fcr_res}{A data.table with FCR results, including 'fcr' and 'cor_fcr' (if applicable)}
#'   \item{adg_info}{The original ADG info data.table}
#'   \item{adg_data}{The original ADG data}
#'   \item{adfi_info}{The original ADFI info data.table}
#'   \item{adfi_data}{The original ADFI data}
#'
#' @note 
#' - This function assumes that 'adg_info' and 'adfi_info' are data.tables with columns 'responder' and 'location'.
#' - The function expects 'corrected_dfi' and 'lm_slope' columns in the joined data for FCR calculation.
#' - The corrected FCR calculation is only performed if a 'stage' column is present in the joined data.
#' - The variables 'min_weight_cut' and 'max_weight_cut' should be defined in the global environment or passed as parameters for the corrected FCR calculation.
#'
#' @export

# FCR results get
fcr_get <- function(adg_res, adfi_res) {
  
  # Parameter checks
  if (!is.list(adg_res) || !all(c("adg_info", "adg_data") %in% names(adg_res))) {
    stop("adg_res must be the output from adg_get() function")
  }
  if (!is.list(adfi_res) || !all(c("adfi_info", "adfi_data") %in% names(adfi_res))) {
    stop("adfi_res must be the output from adfi_get() function")
  }
  if (!data.table::is.data.table(adg_res$adg_info) || !data.table::is.data.table(adfi_res$adfi_info)) {
    stop("adg_info and adfi_info must be data.tables")
  }
  
  dt1 <- adg_res$adg_info
  dt2 <- adfi_res$adfi_info

  # Set keys for faster joining
  data.table::setkeyv(dt1, c("responder", "location"))
  data.table::setkeyv(dt2, c("responder", "location"))

  # Join the data tables
  joined_dt <- dt1[dt2, nomatch = 0]

  # Calculate FCR
  joined_dt[, fcr := corrected_dfi / lm_slope]

  # Check if 'stage' column exists
  if ("stage" %in% colnames(joined_dt)) {
    # Parse stage values
    stage_values <- as.numeric(unlist(strsplit(unique(joined_dt$stage), "-")))

    # Calculate corrected FCR
    joined_dt[, cor_fcr := fcr +
                (stage_values[1] - min_weight_cut/1000) * 0.005 +
                (stage_values[2] - max_weight_cut/1000) * 0.005]
  }
  
  fcr_results_stats <- fcr_summary(joined_dt)
  
  # Create a new list with all results
  result_list <- list(
    fcr_res = joined_dt,
    adg_info = dt1,
    adg_data = adg_res$adg_data,
    adfi_info = dt2,
    adfi_data = adfi_res$adfi_data,
    fcr_summary = fcr_results_stats
  )

  return(result_list)
}
# Helper function to process data
process_fcr_data <- function(data) {
  # Determine column names
  if ("min_weight_cut" %in% names(data)) {
    weight_cols <- c("min_weight_cut", "max_weight_cut")  # If "min_weight_cut" is a column, set weight columns
  } else if ("min_weight_origin" %in% names(data)) {
    weight_cols <- c("min_weight_origin", "max_weight_origin")  # If "min_weight_origin" is a column, set weight columns
  } else {
    stop("Unknown data format")  # If neither column exists, stop execution with an error message
  }

  common_cols <- c("lm_slope", "test_days", "origin_dfi", "corrected_dfi", "fcr")  # Define common columns

  if ("cor_fcr" %in% names(data)) {
    common_cols <- c(common_cols, "cor_fcr")  # Add "cor_fcr" to common columns if it exists
  }

  # Combine all selected columns
  selected_cols <- c(weight_cols, common_cols)

  # Perform operations on these columns in data
  data <- data[, ..selected_cols]  # Select only the chosen columns from the data

  # Find all columns that contain "weight"
  weight_cols <- grep("weight", names(data), value = TRUE)

  # Divide all values in these columns by 1000
  data[, (weight_cols) := lapply(.SD, function(x) x / 1000), .SDcols = weight_cols]

  return(data)  # Return the processed data
}
# Helper function to calculate statistics
fcr_data_stats <- function(data) {
  # Calculate statistics for each column in the data
  stats <- data[, lapply(.SD, function(x) {
    list(
      N = sum(!is.na(x)),  # Number of non-NA values
      min = min(x, na.rm = TRUE),  # Minimum value, ignoring NA
      max = max(x, na.rm = TRUE),  # Maximum value, ignoring NA
      median = stats::median(x, na.rm = TRUE),  # Median value, ignoring NA
      mean = mean(x, na.rm = TRUE),  # Mean value, ignoring NA
      sd = stats::sd(x, na.rm = TRUE),  # Standard deviation, ignoring NA
      cv = stats::sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)  # Coefficient of variation (CV)
    )
  })]

  return(stats)  # Return the calculated statistics
}
# FCR results summary
fcr_summary <- function(data) {

  # Check if all required columns are present
  weight_cols <- c("min_weight_cut", "max_weight_cut", "min_weight_origin", "max_weight_origin")
  common_cols <- c("lm_slope", "test_days", "origin_dfi", "corrected_dfi", "fcr", "cor_fcr")

  if (any(weight_cols %in% names(data))) {
    required_cols <- c(common_cols, weight_cols)
  } else {
    stop("The 'data' argument is missing required weight columns")
  }

  missing_cols <- setdiff(required_cols, names(data))

  # Process data and calculate statistics
  fcr_stats <- process_fcr_data(data) |>
    fcr_data_stats()

  fcr_summary_data <- data.table::data.table(t(fcr_stats))

  data.table::setnames(fcr_summary_data, c("N", "min", "max", "median", "mean", "sd", "cv"))

  fcr_summary_data$traits <- colnames(fcr_stats)

  data.table::setcolorder(fcr_summary_data, c("traits", setdiff(names(fcr_summary_data), "traits")))

  num_cols <- c("N", "min", "max", "median", "mean", "sd", "cv")
  fcr_summary_data[, (num_cols) := lapply(.SD, function(x) as.numeric(unlist(x))), .SDcols = num_cols]


  fcr_summary_data[, cv := sprintf("%.2f%%", round(cv * 100, 2))][, N := as.character(N)]

  numeric_cols <- names(fcr_summary_data)[sapply(fcr_summary_data, is.numeric)]
  fcr_summary_data[, (numeric_cols) := lapply(.SD, function(x) sprintf("%0.2f", as.numeric(x))), .SDcols = numeric_cols][]

  return(fcr_summary_data)
}
```
  
```{r example-fcr_get}
nedap_csv_data <- mintyr::nedap
adg_results <- adg_get(data = nedap_csv_data)
adfi_results <- adfi_get(data = nedap_csv_data, adg_res = adg_results)
fcr_results <- fcr_get(adg_res = adg_results, adfi_res = adfi_results)
head(fcr_results$fcr_res)
head(fcr_results$fcr_summary)
```
  
  
The `import_csv()` function provides easy importing and consolidation of multiple CSV files, handling the repetitive workflow of loading raw data files. The `adg_get()` function then processes the imported data to calculate average daily gain over desired weight ranges, with data cleaning, outlier detection and growth curve plotting. Building on `adg_get()`, `dfi_get()` focuses on visit feed intake data, correcting errors and computing daily feed intake matched to ADG segments. Finally, `fcr_get()` combines the ADG and DFI results to efficiently compute the key feed conversion ratio metric. Together these four functions provide a streamlined workflow for analysis of pig feed efficiency from raw CSV data to cleaned datasets, growth curves, ADG estimates, DFI calculations and FCR metrics ready for downstream evaluation and modeling. By encapsulating repetitive tasks like data import and cleaning, they allow users to focus on analytical insights rather than coding details. The output at each step can be customized based on analysis needs. These interlocked functions enable easy end-to-end data wrangling and analysis for pig feed intake data, facilitating efficient computation of ADG, DFI and FCR feed efficiency indicators. That's it ! This the end of the documented story of our package.

<!-- 
# Inflate your package

You're one inflate from paper to box.
Build your package from this very Rmd using `fusen::inflate()` 
-->


```{r development-inflate, eval=FALSE}
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_teaching.Rmd", vignette_name = "Basic Usage")
```

<!-- 
- Verify your `"DESCRIPTION"` file has been updated
- Verify your function is in `"R/"` directory
- Verify your test is in `"tests/testthat/"` directory
- Verify this Rmd appears in `"vignettes/"` directory 
-->
